{"cells":[{"cell_type":"markdown","metadata":{"id":"XebDJ3UnS3n3"},"source":["<div>\n","<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"e_-HjrL6S3n5"},"source":["# Lab 5.3.1\n","# *Support Vector Machines*\n","\n","SVMs use linear algebra to find an (n-1)-dimensional boundary that separates classes within an n-dimensional space. In practical terms, this technique provides a conceptually simple way to predict class membership from a set of features.\n","\n","The standard (linear) SVM is immediately applicable to linear classification problems. Furthermore, by applying transformations to the feature space it is possible to tackle nonlinear classification problems. These transforms are called *kernels*."]},{"cell_type":"markdown","metadata":{"id":"azVVNUxHYKej"},"source":["### 1. Load Data\n","\n","Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. n the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n","\n","This database is also available through the UW CS ftp server: ftp ftp.cs.wisc.edu cd math-prog/cpo-dataset/machine-learn/WDBC/\n","\n","Also can be found on UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n","\n","Attribute Information:\n","\n","1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)\n","\n","Ten real-valued features are computed for each cell nucleus:\n","\n","a) radius (mean of distances from center to points on the perimeter) b) texture (standard deviation of gray-scale values) c) perimeter d) area e) smoothness (local variation in radius lengths) f) compactness (perimeter^2 / area - 1.0) g) concavity (severity of concave portions of the contour) h) concave points (number of concave portions of the contour) i) symmetry j) fractal dimension (\"coastline approximation\" - 1)\n","\n","The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n","\n","All feature values are recoded with four significant digits.\n","\n","Missing attribute values: none\n","\n","Class distribution: 357 benign, 212 malignant"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, auc, classification_report\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":1,"metadata":{"ExecuteTime":{"end_time":"2019-05-09T05:13:16.458182Z","start_time":"2019-05-09T05:13:16.454244Z"},"id":"aICmn_7xYKek"},"outputs":[],"source":["breast_cancer_csv = 'breast-cancer-wisconsin-data.csv'"]},{"cell_type":"markdown","metadata":{"id":"FPRqG96QYKen"},"source":["### 2. EDA - ALL DONE in 5.1.1\n","\n","- Explore dataset. Clean data (if required)\n","- Find features to predict class"]},{"cell_type":"markdown","metadata":{"id":"Omwx5vVbYKeo"},"source":["### 3. Logistic Regression Model - ALL DONE in 5.1.1\n","\n","#### 3.1 Use Logistic Regression\n","\n","Use Logistic Regression and examine accuracy score, confusion matrix, classification report for that model.\n","\n","- Define Target, Predictors\n","- Train-Test Split\n","- Evaluate Model"]},{"cell_type":"markdown","metadata":{"id":"Mogg_w8vYKep"},"source":["### 4. Support Vector Machine\n","\n","#### 4.1 Use Support Vector Machine\n","\n","Use Support Vector Machine and examine accuracy score, confusion matrix, classification report for that model.\n","\n","- Define Target, Predictors\n","- Train-Test Split\n","- Evaluate Model"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.956140350877193\n","\n","Confusion Matrix:\n","                  Predicted Benign  Predicted Malignant\n","Actual Benign                   70                    1\n","Actual Malignant                 4                   39\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","      Benign       0.95      0.99      0.97        71\n","   Malignant       0.97      0.91      0.94        43\n","\n","    accuracy                           0.96       114\n","   macro avg       0.96      0.95      0.95       114\n","weighted avg       0.96      0.96      0.96       114\n","\n"]}],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","# Load the dataset\n","data = pd.read_csv('breast-cancer-wisconsin-data.csv')\n","\n","# Define target and predictors\n","target = 'diagnosis'\n","predictors = data.columns.drop(['id', target, 'Unnamed: 32'])\n","\n","# Prepare the data\n","X = data[predictors]\n","y = data[target].map({'M': 1, 'B': 0})  # Map diagnosis to binary values\n","\n","# Train-Test Split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize and train the model\n","svm_model = SVC(kernel='linear', random_state=42)\n","svm_model.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred = svm_model.predict(X_test)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","conf_matrix = confusion_matrix(y_test, y_pred)\n","class_report = classification_report(y_test, y_pred, target_names=['Benign', 'Malignant'])\n","\n","# Display the results\n","print(f\"Accuracy: {accuracy}\")\n","print(\"\\nConfusion Matrix:\")\n","print(pd.DataFrame(conf_matrix, index=['Actual Benign', 'Actual Malignant'], columns=['Predicted Benign', 'Predicted Malignant']))\n","print(\"\\nClassification Report:\")\n","print(class_report)\n"]},{"cell_type":"markdown","metadata":{"id":"fdzQkTb7YKeq"},"source":["### 5. Naive Bayes\n","#### 5.1 Use Naive Bayes\n","\n","Use Naive Bayes and examine accuracy score, confusion matrix, classification report for that model.\n","\n","- Define Target, Predictors\n","- Train-Test Split\n","- Evaluate Model"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.9736842105263158\n","\n","Confusion Matrix:\n","                  Predicted Benign  Predicted Malignant\n","Actual Benign                   71                    0\n","Actual Malignant                 3                   40\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","      Benign       0.96      1.00      0.98        71\n","   Malignant       1.00      0.93      0.96        43\n","\n","    accuracy                           0.97       114\n","   macro avg       0.98      0.97      0.97       114\n","weighted avg       0.97      0.97      0.97       114\n","\n"]}],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","# Load the dataset\n","data = pd.read_csv('breast-cancer-wisconsin-data.csv')\n","\n","# Define target and predictors\n","target = 'diagnosis'\n","predictors = data.columns.drop(['id', target, 'Unnamed: 32'])\n","\n","# Prepare the data\n","X = data[predictors]\n","y = data[target].map({'M': 1, 'B': 0})  # Map diagnosis to binary values\n","\n","# Train-Test Split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize and train the Naive Bayes model\n","nb_model = GaussianNB()\n","nb_model.fit(X_train, y_train)\n","\n","# Make predictions\n","y_pred_nb = nb_model.predict(X_test)\n","\n","# Evaluate the model\n","accuracy_nb = accuracy_score(y_test, y_pred_nb)\n","conf_matrix_nb = confusion_matrix(y_test, y_pred_nb)\n","class_report_nb = classification_report(y_test, y_pred_nb, target_names=['Benign', 'Malignant'])\n","\n","# Display the results\n","print(f\"Accuracy: {accuracy_nb}\")\n","print(\"\\nConfusion Matrix:\")\n","print(pd.DataFrame(conf_matrix_nb, index=['Actual Benign', 'Actual Malignant'], columns=['Predicted Benign', 'Predicted Malignant']))\n","print(\"\\nClassification Report:\")\n","print(class_report_nb)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VoGxthaeYKer"},"source":["### 6 Gridsearch optimal parameters for Logistic Regression and SVM models\n","\n","Is there any difference between accuracy score of Logistic Regression and SVM? Use grid search to find optimal parameter for both these models.\n","\n","> Hyper-parameters are parameters that are not directly learnt within estimators. In scikit-learn they are passed as arguments to the constructor of the estimator classes. Typical examples include C, kernel and gamma for Support Vector Classifier, alpha for Lasso, etc.\n","\n","> It is possible and recommended to search the hyper-parameter space for the best cross validation score.\n","\n","> https://scikit-learn.org/stable/modules/grid_search.html#grid-search\n","\n","**Note:** It'll take time to execute this. After running the cell, wait for result."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n","/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n","import matplotlib.pyplot as plt\n","\n","# Load the dataset\n","data = pd.read_csv('breast-cancer-wisconsin-data.csv')\n","\n","# Define target and predictors\n","target = 'diagnosis'\n","predictors = data.columns.drop(['id', target, 'Unnamed: 32'])\n","\n","# Prepare the data\n","X = data[predictors]\n","y = data[target].map({'M': 1, 'B': 0})  # Map diagnosis to binary values\n","\n","# Train-Test Split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","def grid_search_logistic_regression(X_train, y_train):\n","    param_grid = {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'solver': ['liblinear', 'lbfgs', 'saga']\n","    }\n","    log_reg = LogisticRegression(max_iter=10000)\n","    grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n","    grid_search.fit(X_train, y_train)\n","    return grid_search.best_estimator_, grid_search.best_score_\n","\n","def grid_search_svm(X_train, y_train):\n","    param_grid = {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'kernel': ['linear', 'rbf', 'poly'],\n","        'gamma': ['scale', 'auto']\n","    }\n","    svm = SVC()\n","    grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\n","    grid_search.fit(X_train, y_train)\n","    return grid_search.best_estimator_, grid_search.best_score_\n","\n","# Perform Grid Search\n","best_log_reg, best_score_log_reg = grid_search_logistic_regression(X_train, y_train)\n","best_svm, best_score_svm = grid_search_svm(X_train, y_train)\n","\n","# Train and Evaluate Models\n","best_log_reg.fit(X_train, y_train)\n","best_svm.fit(X_train, y_train)\n","nb_model = GaussianNB()\n","nb_model.fit(X_train, y_train)\n","\n","# Predictions\n","y_pred_log_reg = best_log_reg.predict(X_test)\n","y_pred_svm = best_svm.predict(X_test)\n","y_pred_nb = nb_model.predict(X_test)\n","\n","# Calculate ROC AUC\n","roc_auc_log_reg = roc_auc_score(y_test, best_log_reg.decision_function(X_test))\n","roc_auc_svm = roc_auc_score(y_test, best_svm.decision_function(X_test))\n","roc_auc_nb = roc_auc_score(y_test, nb_model.predict_proba(X_test)[:, 1])\n","\n","# Plot ROC Curves\n","fpr_log_reg, tpr_log_reg, _ = roc_curve(y_test, best_log_reg.decision_function(X_test))\n","fpr_svm, tpr_svm, _ = roc_curve(y_test, best_svm.decision_function(X_test))\n","fpr_nb, tpr_nb, _ = roc_curve(y_test, nb_model.predict_proba(X_test)[:, 1])\n","\n","plt.figure(figsize=(10, 7))\n","plt.plot(fpr_log_reg, tpr_log_reg, label=f'Logistic Regression (AUC = {roc_auc_log_reg:.2f})')\n","plt.plot(fpr_svm, tpr_svm, label=f'SVM (AUC = {roc_auc_svm:.2f})')\n","plt.plot(fpr_nb, tpr_nb, label=f'Naive Bayes (AUC = {roc_auc_nb:.2f})')\n","plt.plot([0, 1], [0, 1], 'k--')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('ROC Curve')\n","plt.legend(loc='lower right')\n","plt.show()\n","\n","print(\"Best Logistic Regression Model:\", best_log_reg)\n","print(\"Best Logistic Regression Score:\", best_score_log_reg)\n","print(\"Best SVM Model:\", best_svm)\n","print(\"Best SVM Score:\", best_score_svm)\n"]},{"cell_type":"markdown","metadata":{"id":"UeqrbsyNYKes"},"source":["#### 6.1 Find Best Estimator For Logistic Regression - DONE ABOVE\n","\n","Find out how these parameters effect model. Find out the best estimator, score. - DONE ABOVE"]},{"cell_type":"code","execution_count":2,"metadata":{"ExecuteTime":{"end_time":"2019-05-09T05:40:29.397881Z","start_time":"2019-05-09T05:40:29.392602Z"},"id":"UkQ9RBQZYKet"},"outputs":[],"source":["lr_params = {\n","    'penalty': ['l1','l2'],\n","    'C': [1, 10, 100]\n","}"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2019-05-09T05:23:14.036840Z","start_time":"2019-05-09T05:23:14.032847Z"},"id":"ioLgY3bxYKev"},"source":["#### 6.2 Find Best Estimator For SVM - - DONE ABOVE\n","\n","Find out how these parameters effect model. Find out the best estimator, score. - - DONE ABOVE"]},{"cell_type":"code","execution_count":3,"metadata":{"ExecuteTime":{"end_time":"2019-05-09T05:40:31.617090Z","start_time":"2019-05-09T05:40:31.612996Z"},"id":"vgi61VpWYKew"},"outputs":[],"source":["svc_params = {\n","    'C': [1, 10, 100],\n","    'gamma': [0.001, 0.0001],\n","    'kernel': ['linear','rbf']\n","}"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2019-05-09T05:23:59.157703Z","start_time":"2019-05-09T05:23:59.153713Z"},"id":"HrS04DfuYKez"},"source":["#### 6.3 Plot the ROC curve for the SVM, Logistic Regressions and Naive Bayes on the same plot - - DONE ABOVE\n","\n","Find out which model performs better."]},{"cell_type":"markdown","metadata":{"id":"WrSrz3AAYKe3"},"source":["### 7. [BONUS] Learning Curve\n","\n","A learning curve compares the validation and training performance of an estimator for varying amounts of training data. If both the validation and training score converge to a value that is too low, we will not benefit much from more training data.\n","\n","Plot \"learning curves\" for the best models of each. This is a great way see how training/testing size affects the scores. Look at the documentation for how to use this function in sklearn.\n","\n","http://scikit-learn.org/stable/modules/learning_curve.html#learning-curves"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-05-09T05:22:19.657638Z","start_time":"2019-05-09T05:22:19.653657Z"},"id":"3Zleg5E-YKe4"},"outputs":[],"source":["# https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score\n","\n","# Load the dataset\n","data = pd.read_csv('breast-cancer-wisconsin-data.csv')\n","\n","# Define target and predictors\n","target = 'diagnosis'\n","predictors = data.columns.drop(['id', target, 'Unnamed: 32'])\n","\n","# Prepare the data\n","X = data[predictors]\n","y = data[target].map({'M': 1, 'B': 0})  # Map diagnosis to binary values\n","\n","# Train-Test Split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","def grid_search_logistic_regression(X_train, y_train):\n","    param_grid = {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'solver': ['liblinear', 'lbfgs', 'saga']\n","    }\n","    log_reg = LogisticRegression(max_iter=10000)\n","    grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n","    grid_search.fit(X_train, y_train)\n","    return grid_search.best_estimator_, grid_search.best_score_\n","\n","def grid_search_svm(X_train, y_train):\n","    param_grid = {\n","        'C': [0.01, 0.1, 1, 10, 100],\n","        'kernel': ['linear', 'rbf', 'poly'],\n","        'gamma': ['scale', 'auto']\n","    }\n","    svm = SVC()\n","    grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\n","    grid_search.fit(X_train, y_train)\n","    return grid_search.best_estimator_, grid_search.best_score_\n","\n","# Perform Grid Search\n","best_log_reg, best_score_log_reg = grid_search_logistic_regression(X_train, y_train)\n","best_svm, best_score_svm = grid_search_svm(X_train, y_train)\n","\n","# Train and Evaluate Models\n","best_log_reg.fit(X_train, y_train)\n","best_svm.fit(X_train, y_train)\n","nb_model = GaussianNB()\n","nb_model.fit(X_train, y_train)\n","\n","# Function to plot learning curve\n","def plot_learning_curve(estimator, title, X, y, cv=5, n_jobs=None, train_sizes=np.linspace(0.1, 1.0, 10)):\n","    plt.figure()\n","    plt.title(title)\n","    plt.xlabel(\"Training examples\")\n","    plt.ylabel(\"Score\")\n","    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n","    train_scores_mean = np.mean(train_scores, axis=1)\n","    train_scores_std = np.std(train_scores, axis=1)\n","    test_scores_mean = np.mean(test_scores, axis=1)\n","    test_scores_std = np.std(test_scores, axis=1)\n","    plt.grid()\n","\n","    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n","                     train_scores_mean + train_scores_std, alpha=0.1,\n","                     color=\"r\")\n","    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n","                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n","    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n","             label=\"Training score\")\n","    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n","             label=\"Cross-validation score\")\n","\n","    plt.legend(loc=\"best\")\n","    return plt\n","\n","# Plot learning curves\n","plot_learning_curve(best_log_reg, \"Learning Curve (Logistic Regression)\", X, y)\n","plot_learning_curve(best_svm, \"Learning Curve (SVM)\", X, y)\n","plot_learning_curve(nb_model, \"Learning Curve (Naive Bayes)\", X, y)\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"tE8SgkpSYKe7"},"source":["**References**\n","\n","[Breast Cancer Wisconsin (Diagnostic) Data Set](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data/downloads/breast-cancer-wisconsin-data.zip/2)\n","\n","[Validation curves: plotting scores to evaluate models](https://scikit-learn.org/stable/modules/learning_curve.html#learning-curves)\n","\n","[In-Depth: Support Vector Machines](https://jakevdp.github.io/PythonDataScienceHandbook/05.07-support-vector-machines.html)\n","\n","[Understanding Support Vector Machine algorithm from examples (along with code)](https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)\n","\n","[Tuning the hyper-parameters of an estimator](https://scikit-learn.org/stable/modules/grid_search.html#grid-search)"]},{"cell_type":"markdown","metadata":{"id":"RERADKgNFq9T"},"source":["\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n","> > > > > > > > > © 2024 Institute of Data\n","\n","\n","---\n","\n","\n","\n","---\n","\n","\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":0}
